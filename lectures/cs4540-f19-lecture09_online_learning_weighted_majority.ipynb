{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\LaTeX \\text{ commands here}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\im}{\\text{im}\\,}\n",
    "\\newcommand{\\norm}[1]{||#1||}\n",
    "\\newcommand{\\inner}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\span}{\\mathrm{span}}\n",
    "\\newcommand{\\proj}{\\mathrm{proj}}\n",
    "\\newcommand{\\OPT}{\\mathrm{OPT}}\n",
    "\\newcommand{\\grad}{\\nabla}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border: 5px solid black\">\n",
    "\n",
    "**Georgia Tech, CS 4540**\n",
    "\n",
    "# L9: Online Learning and Weighted Majority\n",
    "\n",
    "Jake Abernethy\n",
    "\n",
    "Quiz password: opt\n",
    "\n",
    "*Thursday, September 19, 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Online Learning\n",
    "\n",
    "It became clear in the early 90s that you could do learning and prediction *without* using statistical tools. One could prove guarantees for sequential algorithms that would hold on any sequence of data.\n",
    "\n",
    "The seminal paper on this topic was *The Weighted Majority Algorithm* by Littlestone and Warmuth. It was a big idea that has been used in many places, and is a core result in the theory of learning.\n",
    "\n",
    "This idea, plus the *Perceptron Algorithm* from Lecture 1, spawned a number of other ideas in ML. In future lectures we'll be exploring Online Convex Optimization and related tools, that can be used to study lots of other ideas as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Doing as well as the best expert\n",
    "\n",
    "As you saw in your reading, we can imagine a sequential game as follows:\n",
    "\n",
    "* For $t=1,2,3,\\ldots$\n",
    "    + Expert $i$ predicts $x_i^t \\in \\{0,1\\}$, for $i=1, \\ldots, N$\n",
    "    + Algorithm predicts $\\hat y^t \\in \\{0,1\\}$\n",
    "    + Nature reveals the *truth* $y^t \\in \\{0,1\\}$\n",
    "    \n",
    "#### Problem\n",
    "\n",
    "Assume there is a *perfect expert*, i.e. $x_i^t = y^t$ for all $t$. Without knowing $i$ in advance, how can you perform *almost* as well? Come up with an algorithm that makes the smallest number of mistakes possible. (A mistake occurs when $\\hat y^t \\ne y^t$). Note: the Algorithm should make significantly fewer than $N$ mistakes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Halving Algorithm\n",
    "\n",
    "The halving algorithm:\n",
    "+ maintain a pool $C^t \\subset [N]$ of \"correct-so-far\" experts\n",
    "+ let $\\hat y^t$ side with the majority of expert predictions $x_i^t$, but only for $i \\in C^t$\n",
    "**Theorem**: the halving algorithm will make fewer than $\\log_2 N$ mistakes!\n",
    "\n",
    "#### Problem\n",
    "\n",
    "Assume you have $m$ teams playing a league. On each of a sequence of rounds, some pair of teams $(i,j)$ will play each other. There is some unknown permutation $\\pi \\in S_m$ which determines the outcome of each match, so $i$ beats $j$ only when $\\pi(i) > \\pi(j)$. You're a gambler, and you want to know how to bet on who wins. What is the best algorithm to use, and how few mistakes will it make before knowing the exact ranking $\\pi$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What if the best expert makes one mistake?\n",
    "\n",
    "You looked at the Weighted Majority Algorithm in the reading, which can be thought of as a generalization of the Halving Algorithm when there are no perfect experts. But let's consider a simple scenario first, and try to use the Halving Algorithm again.\n",
    "\n",
    "#### Problem\n",
    "\n",
    "Assume you have $N$ experts, and each will give you a prediction on each round, and there are exactly $N$ rounds. There isn't a *perfect* expert, but there is one expert that will make exactly one mistake.\n",
    "\n",
    "Can you find an algorithm that uses the experts' advice to make predictions, and is guaranteed to make no more than $2 \\log_2 N$ mistakes? (*Hint*: Use the Halving Alg!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Smarter: The Weighted Majority Algorithm\n",
    "\n",
    "* Weights: $w_i^1 = 1$, for $i = 1, \\ldots, N$\n",
    "* For $t=1,2,3,\\ldots$\n",
    "    + Expert $i$ predicts $x_i^t \\in \\{0,1\\}$, for $i=1, \\ldots, N$\n",
    "    + Algorithm predicts $\\hat y^t = \\text{round}\\left(\\frac{\\sum_i w_i^t x_i^t}{\\sum_i w_i^t}\\right)$\n",
    "    + After seeing $y^t$, update weights: $w_i^{t+1} = w_i^t(1-\\epsilon)^{\\mathbb{1}[x_i^t \\ne y^t]}$\n",
    "    \n",
    "**Theorem**: Let $M_T$ be num mistakes of WMA after $T$ rounds, and let $M_T(i)$ be the number of mistakes of expert $i$. Then for every expert $i$ we have\n",
    "$$ M_T \\leq 2(1+\\epsilon)M_T(i) + \\frac{2 \\log N}{\\epsilon}$$\n",
    "\n",
    "#### Problem\n",
    "\n",
    "What is the best choice of $\\epsilon$ here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Even Smarter: Randomized Weighted Majority\n",
    "\n",
    "\n",
    "* Weights: $w_i^1 = 1$, for $i = 1, \\ldots, N$\n",
    "* For $t=1,2,3,\\ldots$\n",
    "    + Expert $i$ predicts $x_i^t \\in \\{0,1\\}$, for $i=1, \\ldots, N$\n",
    "    + Algorithm predicts *randomly*: let $\\hat y^t = x_i^t$ with prob. $p_i^t = \\frac{w_i^t }{\\sum_j w_j^t}$\n",
    "    + After seeing $y^t$, update weights: $w_i^{t+1} = w_i^t(1-\\epsilon)^{\\mathbb{1}[x_i^t \\ne y^t]}$\n",
    "    \n",
    "**Theorem**: Let $M_T$ be num mistakes of RWM after $T$ rounds, and let $M_T(i)$ be the number of mistakes of expert $i$. Then for every expert $i$ we have\n",
    "$$ \\mathbb{E}[M_T] \\leq (1+\\epsilon)M_T(i) + \\frac{\\log N}{\\epsilon}$$\n",
    "\n",
    "**Corollary**: If we tune correctly, we see that $\\epsilon = \\sqrt{\\frac{\\log N}{M_T(i)}}$ where $i$ is the index of any expert (which we can choose as the best). Then we have\n",
    "$$ \\mathbb{E}[M_T] - M_T(i) \\leq 2\\sqrt{M_T(i) \\log N}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gambling, revisited\n",
    "\n",
    "#### Problem\n",
    "\n",
    "Now assume we are in the gambling problem we had before, with $m$ teams, and on each round a pair of teams shows up to play a match. This time, however, there is some good permutation $\\pi$, but it's not perfect! There will be at most $K$ matches whose outcome doesn't following the permutation $\\pi$.\n",
    "\n",
    "What's a good algorithm for the gambler? And how many mistakes will she make along the way?\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
