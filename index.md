<div style="text-align:center; border:2px solid #853ad6; color: #b8491e; padding: 20px; margin: 0 40px; font-size: 18px; background-color: #f4e5ff">
<a href="https://forms.gle/7MUG3ZwvHs2YLYBW9">Please fill out the COURSE SURVEY!</a>
</div>

## Course Info

**Lecture Info**
- *Time:* Tuesday/Thursday 4:30-5:45pm
- *Location*: Klaus 2443 (*note:* classroom has changed from CCB!)

**Course Staff**

| Role | Name             | `____@gatech.edu` | Office Hours |
| ---- | ---------------- | ----------------- | ------------ |
| Prof | [Jacob Abernethy](https://www.cc.gatech.edu/~jabernethy9/)  | `prof`        | Tuesdays 10am-11am in Klaus 2134 |
| GTA  | Benjamin Bray    | `benrbray`    | TBD |
| TA   | Shyamal Patel    | `patels`      | Fridays 1-3pm between Klaus 2116 and 2124|
| TA   | Rafael Hanashiro | `rhanashiro3` | Mondays 8-10am between Klaus 2116 and 2124 |


**Communication**

We will be using Piazza for managing questions about lecture material, homeworks, and course logistics.  Please only send us email in the event of a personal emergency or other sensitive matter.  If you email us about anything else, we will likely request that you open a Piazza post.  (Note that you can make private posts on Piazza to avoid revealing homework solutions publicly.)

## Course Description

This is an advanced course on algorithms. That is quite a broad topic, and in particular this semester’s course will focus heavily on **algorithms for machine learning**. We will be especially interested in diving into the following topics:

* Numerical Analysis
* Convex Geometry & Optimization
* Probability & Statistical Inference

While students should have a strong back background in core algorithmic concepts, linear algebra, calculus, and probability, we will review many of these topics early in the course. Students will be required to write code in Python, and we will present much of the material in the course using [Jupyter Notebooks](http://jupyter.org/).

## Hands-on Format

In most courses, students learn about material in class through lecture, and then they practice problem solving on their own by doing homework.  In this course we will do the opposite! Students will be required to read material before each class period, and then arrive in class ready to dive into problem-solving.  This way, students can familiarize themselves with basic definitions and examples at home, and benefit from one-on-one interaction with the course staff during lecture while working through more challenging aspects of the material.  Lecture notes for each day will be posted online at least one week prior to each lecture (with the first week as an exception).

Why do it like this? The lecture format is an outdated way to teach mathematical material, especially for topics such as algorithms and machine learning where it is so easy to play with code and implement ideas. The lecture format also limits the professor’s ability to interact directly with students.

Each class period will have the following structure:

* *(0mins)* Students arrive and organize themselves by sitting with their group
* *(5mins)* Students take a very short and simple quiz on assigned reading material
* *(70mins)* Problem time! Instructor presents a sequence of problems, and students are given 5-10 minutes per problem to try to solve each one with their group. Instructors will move around the classroom to engage with students and answer questions.


## Grading

### Homeworks (35%)

Students are allowed, and indeed encouraged, to work on homework with other students in the course. But when solutions are written up, this should be done alone and without the help of other students. Students are required to specify on their writeups which students that collaborated with. If we find solutions that appear even remotely to have been copied, these will be given a zero and the students will be notified.

### Attendance+Quizzes (15%)

In-class quizzes will be graded generously, and 50% of the credit will be given simply for showing up.  Quizzes will be entered electronically, via a web form, so make sure you have a phone, laptop, or tablet with you in class!  If you don't have access to any of these, please let us know and we will make accomodations.  The grading scheme for quizzes will be:

* 2 points for a correct answer
* 1 point for an incorrect answer
* 0 points for not taking the quiz

Your **five lowest quiz scores will be dropped**, which should be enough to account for quizzes missed due to planned or unplanned absences.

### Midterm Exam (20%)

The date of the midterm is TBD.  More details about the topics and format will be released at a later date.

### Final Exam (30%)

The final exam will take place on **Thursday, December 5, 2019** from **2:40-5:30pm**.  The final exam date is fixed by the university, and we are not at liberty to change it.  

## Reading

Readings will be assigned for you to complete *before each lecture*.  All required reading will either be linked to here or posted to canvas.  You are not required to purchase a textbook for this course, but you may find the following books helpful.

* Boyd & Vandengerbhe, *Convex Optimization* ([Free PDF](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf))
* Trefethen & Bau, *Numerical Linear Algebra*

## Important Dates

| ---- | ----- |
| Tuesday, 8/20/19 | First day of class |
| Friday, 8/23/19  | Schedule change deadline |
| Monday, 9/2/19   | Labor Day |
| Mon 10/14/19 - Tues, 10/15/19 | Fall Recess (no class) |
| Monday, 10/26/19 @ 4pm | Withdrawal Deadline |
| Wed, 11/27/19 - Sun, 11/31/19 | Thanksgiving Break |
| Tuesday, 12/3/19 | Last day of class |
| Thursday, 12/5/19 @ 2:40-5:30pm | **Final Exam** (determined by [final exam matrix](https://registrar.gatech.edu/files/201908_Final_Exam_Matrix.pdf)) |

# Homework

* **Homework #1** Due date Sunday, September 8
* **Homework #2** Due date Sunday, September 22

# Calendar

<iframe src="https://calendar.google.com/calendar/embed?showPrint=0&amp;showCalendars=0&amp;mode=WEEK&amp;height=600&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=oomfeedqhivsotfkgiqofqjd8s%40group.calendar.google.com&amp;color=%23AB8B00&amp;ctz=America%2FNew_York" style="border-width:0" width="800" height="600" frameborder="0" scrolling="no"></iframe>

# Schedule

### Tentative Schedule (*read ahead at your own risk!*)

(Tu 8/20/19) **Lecture #1:  Introduction & Perceptron**
([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture01_welcome_and_introduction.ipynb))


No Required Reading

*Additional Resources*
* CMU 15-859(B), Lecture #4, [The Perceptron Algorithm](https://www.cs.cmu.edu/~avrim/ML10/lect0125.pdf) by Avrim Blum
* Raul Rojas, Neural Networks:  A Systematic Introduction
  * [Ch 4:  Perceptron Learning](https://page.mi.fu-berlin.de/rojas/neural/chapter/K4.pdf)
  * [Ch 5:  Unsupervised Learning and Clustering Algorithms](https://page.mi.fu-berlin.de/rojas/neural/chapter/K5.pdf)


(Th 8/22/19) **Lecture #2:  Review of Linear Algebra & Intro to Numpy**
([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture02_linear-algebra.ipynb))

*Required Preparation before Class*

* Notes, [CS 4540:  Python Basics](https://cs4540-f18.github.io/notes/python-basics)
* Brush up on linear algebra!

*Additional Resources*
* 3blue1brown, [Essence of Linear Algebra](http://www.3blue1brown.com/essence-of-linear-algebra-page/) video series
* [UNSW 2501: Linear Algebra](https://gatech.instructure.com/courses/22666/files/folder/unsw-math2501_linear-algebra-notes) Notes on Canvas
* MIT OCW 18.06 Linear Algebra Lecture Videos



(Tu 8/27/19) **Lecture #3:  Convex Geometry**
([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture03_convex_sets.ipynb))

*Required Preparation before Class*

* Read Boyd & Vandenberghe, [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
	* §2.1 Affine and Convex Sets
	* §2.2 Important Examples (of Affine and Convex Sets)
	* §2.5 Separating & Supporting Hyperplanes

*Additional Resources*

* Stanford EE364a, Lecture #2:  Convex Sets ([Slides](http://web.stanford.edu/class/ee364a/lectures/sets.pdf), [Video](https://www.youtube.com/watch?v=P3W_wFZ2kUo))
* Jeffe, UIUC Computational Geometry, ["Lecture 1:  Convex Hulls"](http://jeffe.cs.illinois.edu/teaching/compgeom/notes/01-convexhull.pdf)
* Lauritzen, *Lectures on Convex Sets*, [Ch 3: Separation](https://www.fmf.uni-lj.si/~lavric/lauritzen.pdf) (nice proof of Farkas lemma)
* ETH Zürich, Approximate Methods in Geometry, ["Chapter 1:  Some Basic Geometry"](https://www.ti.inf.ethz.ch/ew/lehre/ApproxGeom08/notes/01_Basic_Geometry.pdf)
* David L. Finn, Rose-Hullman MA 323, ["Barycentric Coordinates & de Casteljau's Algorithm"](https://www.rose-hulman.edu/~finn/CCLI/Notes/day11.pdf)



(Th 8/29/19) **Lecture #4:  Review of Multivariable Calculus**
([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture04_convexity_multivariable_calculus.ipynb))



*Required Preparation before Class*
* Brush up on single and multivariable calculus!
	* Sequences, series, and limits
	* Chain rule, product rule, quotient rule
	* Mean value theorem, intermediate value theorem
	* Taylor series

*Additional Resources*
* Parr & Howard 2018, ["The Matrix Calculus You Need for Deep Learning"](https://arxiv.org/abs/1802.01528)
* Petersen & Pedersen 2012, ["The Matrix Cookbook"](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)
* 3blue1brown YouTube Channel
	* Essence of Calculus #4, ["Visualizing the chain rule and product rule"](https://www.youtube.com/watch?v=YG15m2VwSjA)
	* Essence of Calculus #6, ["Implicit Differentiation, what's going on here?"](https://www.youtube.com/watch?v=qb40J4N1fa4)
	* ["What they won't teach you in Calculus"](https://www.youtube.com/watch?v=CfW845LNObM&t=241s)

(Tu 9/3/19) **Lecture #5:  Convex Functions & Intro to Optimization**
([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture05_convex-functions.ipynb))

*Required Preparation before Class*

* Boyd & Vandenberghe, [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
	* §3.1 Basic Properties & Examples of Convex Functions
		* Skip §3.1.2 Extended-Value Extensions

*Additional Resources*

* Stanford EE364a, Lecture #3:  Convex Functions ([Slides](http://web.stanford.edu/class/ee364a/lectures/functions.pdf), [Video](https://www.youtube.com/watch?v=kcOodzDGV4c))
* Boyd & Vandenberghe, [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
	* §2.3 Operations that Preserve Convex Sets
	* §3.2 Operations that Preserve Convex Functions
	* §2.5 Separating & Supporting Hyperplanes



(Th 9/5/19) **Lecture #6:  More Calculus & Positive Definite Matrices & Convex Functions** ([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture06_convexity_PSD.ipynb))

*Required Preparation before Class*
* Randal J. Barnes, ["Matrix Differentiation"](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf)


(Tu 9/10/19) **Lecture #7:  Gradient Descent for Convex Functions** ([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture07_gradient-descent.ipynb))


*Required Preparation before Class*
* Sham Kakade, ["Optimization 1: Gradient Descent"](https://courses.cs.washington.edu/courses/cse546/15au/lectures/lecture09_optimization.pdf)
    - Read starting from Section 3, the most important thing is to understand the convergence rate of gradient descent under different assumptions (convex and lipschitz, convex and smooth, strongly convex and smooth)
    
*Additional Resources*

* Sebastian Bubeck 2015, ["Convex Optimization: Algorithms and Complexity"](https://arxiv.org/pdf/1405.4980.pdf)
    - Sections 3.0, 3.1, 3.2, and 3.4
* Jonathan Shewchuk 1994, ["Painless Conjugate Gradient"](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) (Pages 1-17)
	* We (probably) won't cover Conjugate-Gradient, but these notes are a great intro gradient descent.
	* We'll cover the Jacobi method in more detail later, so don't worry too much about §5.2
* Definition of strong convexity in Boyd & Vandenberghe, ["Convex Optimization"](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf) §9.1.2
* Moritz Hardt, UC Berkeley EE227C
	* ["Lecture 2:  Gradient Descent (Non-smooth and smooth)"](https://ee227c.github.io/notes/ee227c-lecture02.pdf)
	* ["Lecture 3:  Gradient Descent (Strongly convex)"](https://ee227c.github.io/notes/ee227c-lecture03.pdf)
	* ["Lecture 4:  Applications of Gradient Descent"](https://ee227c.github.io/notes/ee227c-lecture04.pdf)
* Elad Hazan, ["Introduction to Online Convex Optimization"](http://ocobook.cs.princeton.edu/OCObook.pdf), Chapters 2 & 3





(Tu 9/17/19) **Lecture #8:  Intro to Supervised ML: Least Squares, Logistic Regression, Support Vector Machines** 
([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture08_least_squares_SVM_logistic_regression.ipynb))



*Required Preparation before Class*

* Andrew Ng, [CS229 Lecture Notes 1](http://cs229.stanford.edu/summer2019/cs229-notes1.pdf). You need only read:
	- Pages 1-12, intro to least squares regression
	- Pages 14-19, intro to logistic regression, and Newton's method
* Pedro Felzenszwalb [CS142 Lectures Notes 10](http://cs.brown.edu/people/pfelzens/engn2520/CS1420_Lecture_10.pdf)
	- Focus on the *unconstrained* formulation of the SVM in equation (1)!




(Th 9/19/19) **Lecture #9:  Intro to Online Learning** 
([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture09_online_learning_weighted_majority.ipynb))

*Required Preparation before Class*

* Elad Hazan, [Online Convex Optimization](https://ocobook.cs.princeton.edu/OCObook.pdf) book
	- Chapter 1, pages 3-15, *learning with expert advice*


(Tu 9/24/19) **Lecture #10:  Online Convex Optimization and Online Gradient Descent** [Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture10_online_convex_optimization.ipynb)

*Required Preparation before Class*

* Elad Hazan, [Online Convex Optimization](https://ocobook.cs.princeton.edu/OCObook.pdf) book
	- Chapter 3, pages 39-45, *learning with expert advice*
	
(Th 9/26/19) **Lecture #11:  Matrix Decompositions & SVD** 

*Required Preparation before Class*

* Trefethen & Bau, [Numerical Linear Algebra](https://gatech.instructure.com/courses/49866/files/folder/reading?preview=7744347) (posted to Canvas)
	- Lecture 4: The Singular Value Decomposition (Pages 25-31)
	- Lecture 5: More on the SVD (Pages 32-37)

(Tu 10/1/19) **Lecture #12: OGD, SGD, and rates of optimization** [Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture12_SGD_online_to_batch.ipynb)

*Requires Preparation before Class*

* Elad Hazan, [Online Convex Optimization](https://ocobook.cs.princeton.edu/OCObook.pdf) book
	- Chapter 3, pages 46-48, *learning with expert advice*
	
(Th 10/3/19) **Lecture #13:  Linear Programming Introduction** ([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture13_linear-programming.ipynb))

*Required Preparation before Class*
* Tim Roughgarden, Stanford CS261, [Lecture 7: Linear Programming](http://theory.stanford.edu/~tim/w16/l/l7.pdf) (Pages 1-5)
	* Try to get used to the matrix notation for linear programs!  Think geometrically!
	
(Th 10/17/19) **Lecture #14:  Linear Programming Duality** ([Lecture Slides](https://nbviewer.jupyter.org/github/cs4540-f19/cs4540-f19.github.io/blob/master/lectures/cs4540-f19-lecture14_linear-programming-duality.ipynb))

*Required Preparation before Class*
* Tim Roughgarden, Stanford CS261, [Lecture 8: Linear Programming Duality I](http://theory.stanford.edu/~tim/w16/l/l8.pdf) (Pages 1-6)

*Additional Resources*
* Tim Roughgarden, Stanford CS261, [Lecture 9: Linear Programming Duality II](http://theory.stanford.edu/~tim/w16/l/l9.pdf)
* Jim Burke, University of Washington MATH 407
	* ["Section 1:  Linear Programming"](https://sites.math.washington.edu/~burke/crs/407/notes/section1.pdf)
	* ["Section 2:  Simplex Algorithm"](https://sites.math.washington.edu/~burke/crs/407/notes/section2.pdf)
